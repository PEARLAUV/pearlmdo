{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DeXQ1pOcmhKu",
        "D2dSgG-cnVIe",
        "riXmT3Iok_yq",
        "QuMHSd3wr1xH",
        "kUj-VuSzmFaV",
        "VTIybB8b4ar0",
        "nglie5m7q607",
        "bBMcsg4uoKua",
        "mg9ValMRm_Md",
        "MeoGcnV54YY9",
        "EOzQJSX3JOF9",
        "ZF_OjIRrisDB",
        "AKTg9m9yz7Ib",
        "ZMZVMEOPbLWt",
        "TdtqCDWZAC2f",
        "A-L-dB_GBWNq",
        "YFsfzYHzhbM3",
        "QGxWcHWJKtsq",
        "o8iMrvUXK8Id",
        "8-wtZiZmU329",
        "Ru5D0tZEiV2e"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PEARLAUV/pearlmdo/blob/master/notebooks/jax_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4bE-S8yDALH"
      },
      "source": [
        "# Yet another JAX tutorial \n",
        "\n",
        "Kevin Murphy (murphyk@gmail.com).\n",
        "Last update: September 2021.\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a  version of NumPy that runs fast on CPU, GPU and TPU, by compiling the computational graph to XLA (Accelerated Linear Algebra). It also has an excellent automatic differentiation library, extending the earlier [autograd](https://github.com/hips/autograd) package. This library makes it easy to compute higher order derivatives, gradients of complex functions (e.g., optimize an iterative solver), etc.\n",
        "The JAX interface is almost identical to NumPy (by design), but with some small differences, and lots of additional features.\n",
        "We give a brief introduction below. \n",
        "For more details, see [this list of JAX tutorials](https://github.com/probml/probml-notebooks/blob/main/markdown/jax_tutorials.md)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVqju_6BE5c"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0G3tfDFSs"
      },
      "source": [
        "# Standard Python libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from functools import partial\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "\n",
        "from typing import Tuple, NamedTuple\n",
        "\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9kAsUWYDIOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353f9b77-b98e-435a-a078-46833b74d8ad"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax import random, vmap, jit, grad, value_and_grad, hessian, jacfwd, jacrev\n",
        "print(\"jax version {}\".format(jax.__version__))\n",
        "# Check the jax backend\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax version 0.2.19\n",
            "jax backend gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLfVevcW0F4U"
      },
      "source": [
        "# Hardware accelerators\n",
        "\n",
        "Colab makes it easy to use GPUs and TPUs for speeding up some workflows, especially related to deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9saSIGTn661X"
      },
      "source": [
        "## GPUs\n",
        "\n",
        "Colab offers graphics processing units (GPUs) which can be much faster than CPUs (central processing units), as we illustrate below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExOyfluzIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d240863e-b7ca-4cae-86bd-d92256e6ccc3"
      },
      "source": [
        "# Check if GPU is available and its model, memory ...etc.\n",
        "!nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 11 03:46:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0    29W /  70W |  13610MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4Y-hI3FR8f",
        "outputId": "6eac8528-0ff1-4e3a-ca88-41a070cab224"
      },
      "source": [
        "\n",
        "# Check if JAX is using GPU\n",
        "print(\"jax backend {}\".format(jax.lib.xla_bridge.get_backend().platform))\n",
        "# Check the devices avaiable for JAX\n",
        "jax.devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax backend gpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0, process_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ri2ZN_C7ul5"
      },
      "source": [
        "Let's see how JAX can speed up things like matrix-matrix multiplication.\n",
        "\n",
        "First the numpy/CPU version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UxKzydkDQ8-"
      },
      "source": [
        "# Parameters for the experiment\n",
        "size = int(1e3)\n",
        "number_of_loops=int(1e2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmSu22WZ7nRw"
      },
      "source": [
        "# Standard numpy CPU\n",
        "\n",
        "def f(x=None):\n",
        "  if not isinstance(x, np.ndarray):\n",
        "    x=np.ones((size, size), dtype=np.float32) \n",
        "  return np.dot(x, x.T)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEG-7grMDY1Z",
        "outputId": "70f9dae7-457d-4e7b-f2cf-f3e74c18a4a9"
      },
      "source": [
        "%timeit -o -n $number_of_loops f()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 16.4 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 16.4 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y3VO5oc-PhO",
        "outputId": "7688741b-5168-4ceb-de32-68b2240da4c1"
      },
      "source": [
        "res = _ # get result of last cell\n",
        "time_cpu = res.best\n",
        "print(time_cpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.016404767970000192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJPBadZ88Hmi"
      },
      "source": [
        "Now we look at the JAX version. JAX supports execution on [XLA](https://www.tensorflow.org/xla) devices, which can be CPU, GPU or even TPU. We added that block_until_ready because JAX uses [asynchronous execution](https://jax.readthedocs.io/en/latest/async_dispatch.html) by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ptXbto48AWd"
      },
      "source": [
        "# JAX device execution\n",
        "# https://github.com/google/jax/issues/1598\n",
        "\n",
        "def jf(x=None): \n",
        "  if not isinstance(x, jnp.ndarray):\n",
        "    x=jnp.ones((size, size), dtype=jnp.float32)\n",
        "  return jnp.dot(x, x.T)\n",
        "\n",
        "\n",
        "f_gpu = jit(jf, backend='gpu')\n",
        "f_cpu = jit(jf, backend='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVMO-3wPDxoV",
        "outputId": "b21a9308-182b-4f95-f05b-9f4bf34d4603"
      },
      "source": [
        "# Time the CPU version\n",
        "\n",
        "%timeit -o -n $number_of_loops f_cpu() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 25.7 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 25.7 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWQR_9QMDzcd",
        "outputId": "be7fccab-0b06-45cb-d541-0239058505b4"
      },
      "source": [
        "res = _\n",
        "time_jcpu = res.best\n",
        "print(time_jcpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.025695679049999854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPj1zQ4BD3Mq",
        "outputId": "b00925de-a4b6-4ba6-c062-9a3bf814b743"
      },
      "source": [
        "# Time the GPU version\n",
        "\n",
        "%timeit -o -n $number_of_loops f_gpu().block_until_ready() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 40.61 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "100 loops, best of 5: 485 µs per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 485 µs per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTWn8YMMEWfH",
        "outputId": "c41224fd-6c42-42f1-9dc1-6eb6fb251d1e"
      },
      "source": [
        "res = _\n",
        "time_jgpu = res.best\n",
        "print(time_jgpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00048540602000002764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVLUmoB_9pPV",
        "outputId": "2acfbf6b-d257-477d-96f1-ed53b62f5a11"
      },
      "source": [
        "print('JAX CPU time {:0.6f}, Numpy CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_jcpu, time_cpu, time_cpu/time_jcpu))\n",
        "print('JAX GPU time {:0.6f}, Numpy CPU time {:0.6f}, speedup {:0.6f}'.format(\n",
        "    time_jgpu, time_cpu, time_cpu/time_jgpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX CPU time 0.025696, Numpy CPU time 0.016405, speedup 0.638425\n",
            "JAX GPU time 0.000485, Numpy CPU time 0.016405, speedup 33.795971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReLo-XAQEcNN"
      },
      "source": [
        "In the above example we see that JAX GPU is much faster than Numpy CPU.\n",
        "However we also see that JAX CPU is slower than Numpy CPU - this can happen with simple functions, but usually JAX provides a speedup, even on CPU, if you JIT compile a complex function (see below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1H-_vSCz8r"
      },
      "source": [
        "We can move numpy arrays to the GPU for speed. The result will be transferred back to CPU for printing, saving, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxRFku_C9hR",
        "outputId": "4ea1947d-cb61-42c4-9a72-4b73f5202fb9"
      },
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = np.ones((size, size)).astype(np.float32)\n",
        "print(type(x))\n",
        "%timeit -o -n $number_of_loops f(x)\n",
        "\n",
        "x = device_put(x)\n",
        "print(type(x))\n",
        "%timeit -o -n $number_of_loops jf(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "100 loops, best of 5: 14.8 ms per loop\n",
            "<class 'jaxlib.xla_extension.DeviceArray'>\n",
            "100 loops, best of 5: 452 µs per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 452 µs per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS3g8MKl0U9B"
      },
      "source": [
        "## TPUs\n",
        "\n",
        "We can turn on the tensor processing unit by selecting from the Colab runtime.\n",
        "Everything else \"just works\" as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXVaclfH0Xxg"
      },
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0A6Fjr0Ao2i"
      },
      "source": [
        "If everything is set up correctly, the following command should return a list of 8 TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KErnpw1bAkM3",
        "outputId": "9b322a69-68ce-4560-9071-f4b2cf1508d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "jax.local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0, process_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zsh5DdOF4R1"
      },
      "source": [
        "# Vmap <a class=\"anchor\" id=\"vmap\"></a>\n",
        "\n",
        "We often write a function to process a single vector or matrix, and then want to apply it to a batch of data. Using for loops is slow, and manually batchifying code is complex. Fortunately we can use the `vmap` function, which will map our function across a set of inputs, automatically batchifying it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeXQ1pOcmhKu"
      },
      "source": [
        "## Example: 1d convolution\n",
        "\n",
        "(This example is from the Deepmind tutorial.)\n",
        "\n",
        "Consider standard 1d convolution of two vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Ebbw6qmsk8",
        "outputId": "e788b54e-2f86-4df4-961b-c9e571c491e9"
      },
      "source": [
        "x = jnp.arange(5)\n",
        "w = jnp.array([2., 3., 4.])\n",
        "\n",
        "def convolve(x, w):\n",
        "  output = []\n",
        "  for i in range(1, len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2], w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(x, w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([11., 20., 29.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-34EQoIsmx8m"
      },
      "source": [
        "Now suppose we want to convolve multiple vectors with multiple kernels. The simplest way is to use a for loop, but this is slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8NJ-ZQ3m3-K",
        "outputId": "4847ca11-2e53-4cfa-97c6-192f79ede6af"
      },
      "source": [
        "xs = jnp.stack([x, x])\n",
        "ws = jnp.stack([w, w])\n",
        "\n",
        "def manually_batched_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(xs.shape[0]):\n",
        "    output.append(convolve(xs[i], ws[i]))\n",
        "  return jnp.stack(output)\n",
        "\n",
        "manually_batched_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQzNQCpGnGEu"
      },
      "source": [
        "We can manually vectorize the code, but it is complex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1TUBdS1nJD7",
        "outputId": "a57e1cc9-90a0-4b2c-dd44-ee5d7c6b0a82"
      },
      "source": [
        "def manually_vectorised_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(1, xs.shape[-1] -1):\n",
        "    output.append(jnp.sum(xs[:, i-1:i+2] * ws, axis=1))\n",
        "  return jnp.stack(output, axis=1)\n",
        "\n",
        "manually_vectorised_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SE-gk4ZnNLS"
      },
      "source": [
        "Fortunately vmap can do this for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEMqO2k7nQdR",
        "outputId": "164ea7e3-fb02-448f-bb5d-e5db84a136c5"
      },
      "source": [
        "auto_batch_convolve = jax.vmap(convolve)\n",
        "\n",
        "auto_batch_convolve(xs, ws)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2dSgG-cnVIe"
      },
      "source": [
        "## Axes\n",
        "\n",
        "By default, vmap vectorizes over the first axis of each of its inputs. If the first argument has a batch and the second does not, ,specify `in_axes=[0,None]`, so the second argument is not vectorized over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuK4kBGsnkS-",
        "outputId": "66063bda-47d1-40aa-c05c-13e1afc4c6ba"
      },
      "source": [
        "jax.vmap(convolve, in_axes=[0, None])(xs, w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 20., 29.],\n",
              "             [11., 20., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8yq14nqoYXX"
      },
      "source": [
        "We can also vectorize over other dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRLJNGQoctn",
        "outputId": "6fd8a299-af48-4a4f-83fd-977fe1e227d8"
      },
      "source": [
        "\n",
        "print(xs.shape)\n",
        "xst = jnp.transpose(xs)\n",
        "print(xst.shape)\n",
        "\n",
        "wst = jnp.transpose(ws)\n",
        "\n",
        "auto_batch_convolve_v2 = jax.vmap(convolve, in_axes=1, out_axes=1)\n",
        "auto_batch_convolve_v2(xst, wst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5)\n",
            "(5, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[11., 11.],\n",
              "             [20., 20.],\n",
              "             [29., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riXmT3Iok_yq"
      },
      "source": [
        "## Example: logistic regression\n",
        "\n",
        "We now give another example, using binary logistic regression.\n",
        "Let us start with a predictor for a single example\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XAMcxMsF0-Q"
      },
      "source": [
        "\n",
        "D = 2\n",
        "N = 3\n",
        "\n",
        "w = np.random.normal(size=(D,))\n",
        "X = np.random.normal(size=(N,D))\n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnAZ27ziBl2z",
        "outputId": "c984de86-b842-41b3-ecff-5b15988be462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(predict_single(X[0,:])) # works\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23207036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3tRFRhzBnlo",
        "outputId": "45305636-5a57-4c0e-9aaf-fa00c967ac27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "\n",
        "print(predict_single(X)) # fails"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-853aeac052fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-e29aadf4f504>\u001b[0m in \u001b[0;36mpredict_single\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <(D) , (D)> = (1) # inner product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   4194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4195\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4198\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mb_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    666\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     raise TypeError(\"Incompatible shapes for dot: got {} and {}.\".format(\n\u001b[0;32m--> 668\u001b[0;31m         lhs.shape, rhs.shape))\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Incompatible shapes for dot: got (2,) and (3, 2)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMzBunJZpFdt"
      },
      "source": [
        "We can manually vectorize the code by remembering the shapes, so \n",
        "$X w$ multiplies each row of $X$ with $w$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD11HZSpotEM",
        "outputId": "ba3982ba-bc7a-4eb0-c229-fc9348f4e073"
      },
      "source": [
        "def predict_batch(X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "print(predict_batch(X)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.232 0.121 0.878]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZSksj-1GZcU"
      },
      "source": [
        "But it easier to use vmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suWNVZ5uphgc",
        "outputId": "40effd30-7af8-454a-d885-c511cb6d1e17"
      },
      "source": [
        "print(vmap(predict_single)(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.232 0.121 0.878]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vJjjuKV9D-y"
      },
      "source": [
        "## Failure cases\n",
        "\n",
        "Vmap requires that the shapes of all the variables that are created by the function that is being mapped are the same for all values of the input arguments, as explained [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html). So vmap cannot be used to do any kind of embarassingly parallel task. Below we give a simple example of where this fails, since internally we create a vector whose length depends on the input 'length'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz_YLnDV9VlC",
        "outputId": "57cd68a9-e878-4bcf-8b0c-6268e04c7ee2"
      },
      "source": [
        "def example_fun(length, val=4):\n",
        "  return jnp.sum(jnp.ones((length,)) * val)\n",
        "\n",
        "xs = jnp.arange(1,10)\n",
        "\n",
        "# Python map works fine\n",
        "v = list(map(example_fun, xs))\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DeviceArray(4., dtype=float32), DeviceArray(8., dtype=float32), DeviceArray(12., dtype=float32), DeviceArray(16., dtype=float32), DeviceArray(20., dtype=float32), DeviceArray(24., dtype=float32), DeviceArray(28., dtype=float32), DeviceArray(32., dtype=float32), DeviceArray(36., dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM-v2F199jl7"
      },
      "source": [
        "The following fails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "Bvq2JSsT9lh0",
        "outputId": "a6654d03-44b1-4828-89bc-92c70ed6a1a5"
      },
      "source": [
        "v = vmap(example_fun)(xs)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-7fe93a3635da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m     ).call_wrapped(*args_flat)\n\u001b[0m\u001b[1;32m   1288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4138480dd486>\u001b[0m in \u001b[0;36mexample_fun\u001b[0;34m(length, val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexample_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m   3187\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3188\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype)\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \"\"\"\n\u001b[0;32m-> 1595\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0m_invalid_shape_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n       batch_dim = 0,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-7fe93a3635da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_fun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4138480dd486>\u001b[0m in \u001b[0;36mexample_fun\u001b[0;34m(length, val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexample_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m   3186\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3187\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3188\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)\n       batch_dim = 0,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X30m2EagpHSS"
      },
      "source": [
        "# Stochastics\n",
        "\n",
        "JAX is designed to be deterministic, but in some cases, we want to introduce randomness in a controlled way, and to reason about it. We discuss this below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOZdeYBKjWc"
      },
      "source": [
        "## Random number generation\n",
        "\n",
        "One of the biggest differences from NumPy is the way Jax treates pseudo random number generation (PRNG).\n",
        "This is because Jax does not maintain any global state, i.e., it is purely functional.\n",
        "This design \"provides reproducible results invariant to compilation boundaries and backends,\n",
        "while also maximizing performance by enabling vectorized generation and parallelization across random calls\"\n",
        "(to quote [the official page](https://github.com/google/jax#a-brief-tour)).\n",
        "\n",
        "For example, consider this Numpy snippet. Each call to np.random.uniform updates the global state. The value of foo() is therefore only guaranteed to give the same result every time if we evaluate bar() and baz() in the same order (eg left to right). This is why foo1 and foo2 give different answers even though mathematically they shouldn't (we cannot just substitute in the value of a variable and derive the result, so we are violating \"referential transparency\").\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-uNUHTyuamz",
        "outputId": "e06e04c9-9da0-45dd-de6e-496078970d9e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def bar(): return np.random.uniform(size=(3))\n",
        "def baz(): return np.random.uniform(size=(3))\n",
        "\n",
        "def foo(seed): \n",
        "  np.random.seed(seed)\n",
        "  return bar() + 2*baz()\n",
        "\n",
        "def foo1(seed): \n",
        "  np.random.seed(seed)\n",
        "  a = bar()\n",
        "  b =  2*baz()\n",
        "  return a+b\n",
        "\n",
        "def foo2(seed): \n",
        "  np.random.seed(seed)\n",
        "  a = 2*baz() \n",
        "  b = bar()\n",
        "  return a+b\n",
        "\n",
        "seed = 0\n",
        "\n",
        "print(foo(seed))\n",
        "print(foo1(seed))\n",
        "print(foo2(seed))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.639 1.562 1.895]\n",
            "[1.639 1.562 1.895]\n",
            "[1.643 1.854 1.851]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5GLmTGJuqG_"
      },
      "source": [
        "\n",
        "\n",
        "Jax may evaluate parts of expressions such as `bar() + baz()` in parallel, which would violate reproducibility. To prevent this, the user must pass in an explicit PRNG key to every function that requires a source of randomness. Using the same key will give the same results. See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcTYfznjKeHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78cc94a-12d4-4891-f08f-6cfa5469bb64"
      },
      "source": [
        "\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]\n",
        "print(random.normal(key, shape=(3,)))  # [ 1.81608593 -0.48262325  0.33988902]  ## identical results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.816 -0.483  0.34 ]\n",
            "[ 1.816 -0.483  0.34 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfARZPjrvd_z"
      },
      "source": [
        "When generating independent samples, it is important to use different keys, to ensure results are not correlated. We can do this by *splitting* the key into the the 'master' key (which will be used in later parts of the code via splitting), and the 'subkey', which is used temporarily to generate randomness and then thrown away, as we illustrate below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUOY3pkpvnUN",
        "outputId": "1f2b257e-0886-40ef-8abb-2ac7718679cb"
      },
      "source": [
        "# To make a new key, we split the current key into two pieces.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [ 1.1378783  -1.22095478 -0.59153646]\n",
        "\n",
        "# We can continue to split off new pieces from the global key.\n",
        "key, subkey = random.split(key)\n",
        "print(random.normal(subkey, shape=(3,)))  # [-0.06607265  0.16676566  1.17800343]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.138 -1.221 -0.592]\n",
            "[-0.066  0.167  1.178]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oETNlM1RzWXK"
      },
      "source": [
        "We now reimplement the numpy example in Jax and show that we get the result no matter the order of evaluation of bar and baz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTasDPbRwDrk",
        "outputId": "1a9426b4-e9d2-4428-fb8c-499621454e19"
      },
      "source": [
        "\n",
        "def bar(key): \n",
        "  return jax.random.uniform(key,shape=(3,))\n",
        "\n",
        "def baz(key):\n",
        "  return jax.random.uniform(key,shape=(3,))\n",
        "\n",
        "def foo(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  return bar(subkey1) + 2 * baz(subkey2)\n",
        "\n",
        "def foo1(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  a = bar(subkey1) \n",
        "  b =  2 * baz(subkey2)\n",
        "  return a+b\n",
        "\n",
        "def foo2(key): \n",
        "  subkey1, subkey2 = random.split(key, num=2) \n",
        "  a = 2 * baz(subkey2)\n",
        "  b = bar(subkey1)\n",
        "  return a+b\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "key, subkey = random.split(key) \n",
        "print(foo(subkey))\n",
        "print(foo1(subkey))\n",
        "print(foo2(subkey))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.079 2.002 1.089]\n",
            "[2.079 2.002 1.089]\n",
            "[2.079 2.002 1.089]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAlQEKHOzfUG"
      },
      "source": [
        "In Jax (but not in python),  a random draw of N samples in parallel will not give the same results as N draws of individual samples, as we show below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLRJkXRvz3YL",
        "outputId": "ced0dbe6-6420-4aac-eed3-94dcb50b03ea"
      },
      "source": [
        "key = random.PRNGKey(42)\n",
        "subkeys = random.split(key, 3)\n",
        "sequence = np.stack([jax.random.normal(subkey) for subkey in subkeys])\n",
        "print(\"individually:\", sequence)\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(\"all at once: \", jax.random.normal(key, shape=(3,)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "individually: [-0.048  0.108 -1.223]\n",
            "all at once:  [ 0.187 -1.281 -1.559]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr1Kw7vyz5u-",
        "outputId": "89c7c0d7-f7cd-4140-aba4-ccdfaa1e9fa3"
      },
      "source": [
        "np.random.seed(0)\n",
        "sequence = np.stack([np.random.normal() for i in range(3)])\n",
        "print(\"individually:\", sequence)\n",
        "\n",
        "np.random.seed(0)\n",
        "print(\"all at once: \", np.random.normal(size=(3,)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "individually: [1.764 0.4   0.979]\n",
            "all at once:  [1.764 0.4   0.979]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFlhTJUpSZJ"
      },
      "source": [
        "## Probability distributions\n",
        "\n",
        "The [distrax library](https://github.com/deepmind/distrax) is a JAX-native implementation of some parts of  the distrbitions library from [Tensorflow Probabilty (TFP)](https://www.tensorflow.org/probability). The main advantage is that the distrax source code is much easier to read and understand. For distributions not in distrax, it is possible to use TFP instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fUEQe7YEPvL"
      },
      "source": [
        "Here is a brief example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkWvxSlkEWGG"
      },
      "source": [
        "%%capture\n",
        "!pip install git+git://github.com/deepmind/distrax.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P4Bje7eERei",
        "outputId": "feb9c46f-a875-431c-b05e-71e568cd45ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import distrax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "key = jax.random.PRNGKey(1234)\n",
        "mu = jnp.array([-1., 0., 1.])\n",
        "sigma = jnp.array([0.1, 0.2, 0.3])\n",
        "\n",
        "dist_distrax = distrax.MultivariateNormalDiag(mu, sigma)\n",
        "dist_tfp = tfd.MultivariateNormalDiag(mu, sigma)\n",
        "\n",
        "samples = dist_distrax.sample(seed=key)\n",
        "\n",
        "# Both print 1.775\n",
        "print(dist_distrax.log_prob(samples))\n",
        "print(dist_tfp.log_prob(samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7750063\n",
            "1.7750063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2RYhnNG9hh"
      },
      "source": [
        "# Autograd <a class=\"anchor\" id=\"AD\"></a>\n",
        "\n",
        "In this section, we illustrate automatic differentation using JAX.\n",
        "For details, see see  [this video](https://www.youtube.com/watch?v=wG_nF1awSSY&t=697s)  or [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuMHSd3wr1xH"
      },
      "source": [
        "## Derivatives\n",
        "\n",
        "We can compute $(\\nabla f)(x)$ using `grad(f)(x)`. For example, consider\n",
        "\n",
        "\n",
        "$f(x) = x^3 + 2x^2 - 3x + 1$\n",
        "\n",
        "$f'(x) = 3x^2 + 4x -3$\n",
        "\n",
        "$f''(x) = 6x + 4$\n",
        "\n",
        "$f'''(x) = 6$\n",
        "\n",
        "$f^{iv}(x) = 0$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYTM5MPmr3C0",
        "outputId": "efbf2fc3-7ba8-443a-ba3a-d991f842231d"
      },
      "source": [
        "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
        "\n",
        "dfdx = jax.grad(f)\n",
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "d4fdx = jax.grad(d3fdx)\n",
        "\n",
        "print(dfdx(1.))\n",
        "print(d2fdx(1.))\n",
        "print(d3fdx(1.))\n",
        "print(d4fdx(1.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "10.0\n",
            "6.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUj-VuSzmFaV"
      },
      "source": [
        "## Partial derivatives\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x,y) &= x^2 + y \\\\\n",
        "\\frac{\\partial f}{\\partial x} &= 2x \\\\\n",
        "\\frac{\\partial f}{\\partial y} &= 1 \n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0hW7fqfmR1c",
        "outputId": "b3aa0bb1-0dd5-493a-98a1-1977c16e11a3"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2 + y\n",
        "\n",
        "# Partial derviatives\n",
        "x = 2.0; y= 3.0;\n",
        "v, gx = value_and_grad(f, argnums=0)(x,y)\n",
        "print(v)\n",
        "print(gx)\n",
        "\n",
        "gy = grad(f, argnums=1)(x,y)\n",
        "print(gy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0\n",
            "4.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIybB8b4ar0"
      },
      "source": [
        "## Gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb0gZ_1HBEyC"
      },
      "source": [
        "Linear function: multi-input, scalar output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; a) &= a^T x\\\\\n",
        "\\nabla_x f(x;a) &= a\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYqFEs04vkV",
        "outputId": "6c2e1e0a-3ba7-4da5-94ba-f03267379093"
      },
      "source": [
        "\n",
        "\n",
        "def fun1d(x):\n",
        "    return jnp.dot(a, x)[0]\n",
        "\n",
        "Din = 3; Dout = 1;\n",
        "a = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "\n",
        "g = grad(fun1d)(x)\n",
        "assert np.allclose(g, a)\n",
        "\n",
        "\n",
        "# It is often useful to get the function value and gradient at the same time\n",
        "val_grad_fn = jax.value_and_grad(fun1d)\n",
        "v, g = val_grad_fn(x)\n",
        "print(v)\n",
        "print(g)\n",
        "assert np.allclose(v, fun1d(x))\n",
        "assert np.allclose(a, g)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9472518\n",
            "[ 2.241  1.868 -0.977]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbgiqkF6BL1E"
      },
      "source": [
        "Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6hkEYxV5EIx"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)\n",
        "assert np.allclose(Jf, A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5d-D7XBU9Y"
      },
      "source": [
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x f(x;A) &= (A+A^T) x\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URZeX8PBbhl"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "g = grad(quadfun)(x)\n",
        "assert np.allclose(g, jnp.dot(A+A.T, x))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ZOhDeqCXu3"
      },
      "source": [
        "Chain rule applied to sigmoid function.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu(x;w) &=\\sigma(w^T x) \\\\\n",
        "\\nabla_w \\mu(x;w) &= \\sigma'(w^T x) x \\\\\n",
        "\\sigma'(a) &= \\sigma(a) * (1-\\sigma(a)) \n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q5VfLXLB7rv",
        "outputId": "f72a1543-bb65-4a23-a921-5841d22b0079"
      },
      "source": [
        "\n",
        "\n",
        "D = 4\n",
        "w = np.random.normal(size=(D,))\n",
        "x = np.random.normal(size=(D,))\n",
        "y = 0 \n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "def mu(w): return sigmoid(jnp.dot(w,x))\n",
        "def deriv_mu(w): return mu(w) * (1-mu(w)) * x\n",
        "deriv_mu_jax =  grad(mu)\n",
        "\n",
        "print(deriv_mu(w))\n",
        "print(deriv_mu_jax(w))\n",
        "\n",
        "assert np.allclose(deriv_mu(w), deriv_mu_jax(w), atol=1e-3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.13  -0.017 -0.072  0.031]\n",
            "[-0.13  -0.017 -0.072  0.031]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nglie5m7q607"
      },
      "source": [
        "## Auxiliary return values\n",
        "\n",
        "A function can return its value and other auxiliary results; the latter are not differentiated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHz6zrC9qVjT",
        "outputId": "d0a974d1-1e0f-4697-e700-2760cf6488be"
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2+y, 42\n",
        "\n",
        "x = 2.0\n",
        "y = 3.0\n",
        "(v,aux), g = value_and_grad(f, has_aux=True)(x,y)\n",
        "print(v)\n",
        "print(aux)\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.0\n",
            "42\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBMcsg4uoKua"
      },
      "source": [
        "## Jacobians\n",
        "\n",
        "\n",
        "Example: Linear function: multi-input, multi-output.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= A x \\\\\n",
        "\\frac{\\partial f(x;A)}{\\partial x} &= A\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPilm5H3oWcy"
      },
      "source": [
        "# We construct a multi-output linear function.\n",
        "# We check forward and reverse mode give same Jacobians.\n",
        "\n",
        "\n",
        "def fun(x):\n",
        "    return jnp.dot(A, x)\n",
        "\n",
        "Din = 3; Dout = 4;\n",
        "A = np.random.normal(size=(Dout, Din))\n",
        "x = np.random.normal(size=(Din,))\n",
        "Jf = jacfwd(fun)(x)\n",
        "Jr = jacrev(fun)(x)\n",
        "assert np.allclose(Jf, Jr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg9ValMRm_Md"
      },
      "source": [
        "## Hessians\n",
        "\n",
        "Quadratic form.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x;A) &= x^T A x \\\\\n",
        "\\nabla_x^2 f(x;A) &= A + A^T\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leW9lqvinDsM"
      },
      "source": [
        "\n",
        "D = 4\n",
        "A = np.random.normal(size=(D,D))\n",
        "x = np.random.normal(size=(D,))\n",
        "\n",
        "quadfun = lambda x: jnp.dot(x, jnp.dot(A, x))\n",
        "\n",
        "\n",
        "H1 = hessian(quadfun)(x)\n",
        "assert np.allclose(H1, A+A.T)\n",
        "\n",
        "def my_hessian(fun):\n",
        "  return jacfwd(jacrev(fun))\n",
        "\n",
        "H2 = my_hessian(quadfun)(x)\n",
        "assert np.allclose(H1, H2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeoGcnV54YY9"
      },
      "source": [
        "## Example: Binary logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isql2l4MGfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bfc6daf-5f60-439e-a51a-9c898661eb70"
      },
      "source": [
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(w, x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product\n",
        "  \n",
        "def predict_batch(w, X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "# negative log likelihood\n",
        "def loss(weights, inputs, targets):\n",
        "    preds = predict_batch(weights, inputs)\n",
        "    logprobs = jnp.log(preds) * targets + jnp.log(1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(logprobs)\n",
        "\n",
        "\n",
        "D = 2\n",
        "N = 3\n",
        "w = jax.random.normal(key, shape=(D,))\n",
        "X = jax.random.normal(key, shape=(N,D))\n",
        "y = jax.random.choice(key, 2, shape=(N,)) # uniform binary labels\n",
        "#logits = jnp.dot(X, w)\n",
        "#y = jax.random.categorical(key, logits)\n",
        "\n",
        "print(loss(w, X, y))\n",
        "\n",
        "# Gradient function\n",
        "grad_fun = grad(loss)\n",
        "\n",
        "# Gradient of each example in the batch - 2 different ways\n",
        "grad_fun_w = partial(grad_fun, w)\n",
        "grads = vmap(grad_fun_w)(X,y)\n",
        "print(grads)\n",
        "assert grads.shape == (N,D)\n",
        "\n",
        "grads2 = vmap(grad_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "assert np.allclose(grads, grads2)\n",
        "\n",
        "# Gradient for entire batch\n",
        "grad_sum = jnp.sum(grads, axis=0)\n",
        "assert grad_sum.shape == (D,)\n",
        "print(grad_sum)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.384371\n",
            "[[ 0.596 -0.621]\n",
            " [ 0.315  0.837]\n",
            " [-0.233 -0.448]]\n",
            "[ 0.677 -0.232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BaHdT4Gj6W",
        "outputId": "2506d250-8e6c-4a5a-f22f-90dbd44091ed"
      },
      "source": [
        "# Textbook implementation of gradient\n",
        "def NLL_grad(weights, batch):\n",
        "    X, y = batch\n",
        "    N = X.shape[0]\n",
        "    mu = predict_batch(weights, X)\n",
        "    g = jnp.sum(jnp.dot(jnp.diag(mu - y), X), axis=0)\n",
        "    return g\n",
        "\n",
        "grad_sum_batch = NLL_grad(w, (X,y))\n",
        "print(grad_sum_batch)\n",
        "assert np.allclose(grad_sum, grad_sum_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.677 -0.232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_4lRrHgpLbG",
        "outputId": "f4757ea9-368f-43bc-de65-8b32e165357b"
      },
      "source": [
        "# We can also compute Hessians, as we illustrate below.\n",
        "\n",
        "hessian_fun = hessian(loss)\n",
        "\n",
        "# Hessian on one example\n",
        "H0 = hessian_fun(w, X[0,:], y[0])\n",
        "print('Hessian(example 0)\\n{}'.format(H0))\n",
        "\n",
        "# Hessian for batch\n",
        "Hbatch = vmap(hessian_fun, in_axes=(None, 0, 0))(w, X, y) \n",
        "print('Hbatch shape {}'.format(Hbatch.shape))\n",
        "\n",
        "Hbatch_sum = jnp.sum(Hbatch, axis=0)\n",
        "print('Hbatch sum\\n {}'.format(Hbatch_sum))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hessian(example 0)\n",
            "[[ 0.185 -0.193]\n",
            " [-0.193  0.201]]\n",
            "Hbatch shape (3, 2, 2)\n",
            "Hbatch sum\n",
            " [[0.357 0.225]\n",
            " [0.225 1.239]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcJvgukUpWWE"
      },
      "source": [
        "# Textbook implementation of Hessian\n",
        "\n",
        "def NLL_hessian(weights, batch):\n",
        "  X, y = batch\n",
        "  mu = predict_batch(weights, X)\n",
        "  S = jnp.diag(mu * (1-mu))\n",
        "  H = jnp.dot(jnp.dot(X.T, S), X)\n",
        "  return H\n",
        "\n",
        "H2 = NLL_hessian(w, (X,y) )\n",
        "\n",
        "assert np.allclose(Hbatch_sum, H2, atol=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzQJSX3JOF9"
      },
      "source": [
        "## Vector Jacobian Products (VJP) and Jacobian Vector Products (JVP)\n",
        "\n",
        "Suppose $f1(x) = W x$ for fixed W, so $J(x) = W$, and $u^T J(x) = W^T u$.\n",
        "Instead of computing $J$ explicitly and then multiplying by $u$, \n",
        "wecan do this in one operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Lpmntn2JREG",
        "outputId": "42afa7b7-4563-4a7b-84a3-462cd46c6117"
      },
      "source": [
        "n = 3; m = 2;\n",
        "W = jax.random.normal(key, shape=(m,n))\n",
        "x = jax.random.normal(key, shape=(n,))\n",
        "u = jax.random.normal(key, shape=(m,))\n",
        "\n",
        "def f1(x): return jnp.dot(W,x)\n",
        "\n",
        "J1 = jacfwd(f1)(x)\n",
        "print(J1.shape)\n",
        "\n",
        "assert np.allclose(J1, W)\n",
        "tmp1 = jnp.dot(u.T, J1)\n",
        "print(tmp1)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f1, x)\n",
        "\n",
        "tmp2 = jvp_fun(u)\n",
        "\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.dot(W.T, u)\n",
        "assert np.allclose(tmp1, tmp3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "[ 0.888 -0.538 -0.539]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYSC6DMOO3IS"
      },
      "source": [
        "Suppose\n",
        "$f2(W) = W x$ for fixed $x$.\n",
        "Now $J(W) = \\text{something complex}$,\n",
        "but $u^T J(W) = J(W)^T u = u x^T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8l3StJdO1r1",
        "outputId": "7dbcf8a1-714d-4de6-bca4-7cbd969aefdb"
      },
      "source": [
        "\n",
        "def f2(W): return jnp.dot(W,x)\n",
        "\n",
        "J2 = jacfwd(f2)(W)\n",
        "print(J2.shape)\n",
        "\n",
        "tmp1 = jnp.dot(u.T, J2)\n",
        "print(tmp1)\n",
        "print(tmp1.shape)\n",
        "\n",
        "(val, jvp_fun) = jax.vjp(f2, W)\n",
        "tmp2 = jvp_fun(u)\n",
        "assert np.allclose(tmp1, tmp2)\n",
        "\n",
        "tmp3 = np.outer(u, x)\n",
        "assert np.allclose(tmp1, tmp3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 3)\n",
            "[[-0.009 -0.032 -0.474]\n",
            " [ 0.005  0.019  0.286]]\n",
            "(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_OjIRrisDB"
      },
      "source": [
        "## Stop-gradient\n",
        "\n",
        "Sometimes we want to take the gradient of a complex expression wrt some parameters $\\theta$, but treating $\\theta$ as a constant for some parts of the expression. For example, consider the TD(0) update in reinforcement learning, which has the following form:\n",
        "\n",
        "\n",
        "$\\Delta \\theta = (r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})) \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        "where $s$ is the state, $r$ is the reward, and $v$ is the value function.\n",
        "This update is not the gradient of any loss function.\n",
        "However, if the dependency of the target $r_t + v_{\\theta}(s_t)$ on the parameter $\\theta$ is ignored, it can be written as the gradient of the pseudo loss function\n",
        "\n",
        "$L(\\theta) = [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})]^2$\n",
        "\n",
        "since\n",
        "\n",
        "$\\nabla_{\\theta} L(\\theta) = 2 [r_t + v_{\\theta}(s_t) - v_{\\theta}(s_{t-1})] \\nabla v_{\\theta}(s_{t-1})$\n",
        "\n",
        ". We can implement this in JAX using `stop_gradient`, as we show below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9cprxb9jbsK",
        "outputId": "5778acd9-d2fe-4b79-957b-0a81e68419c3"
      },
      "source": [
        "def td_loss(theta, s_prev, r_t, s_t):\n",
        "  v_prev = value_fn(theta, s_prev)\n",
        "  target = r_t + value_fn(theta, s_t)\n",
        "  return 0.5*(jax.lax.stop_gradient(target) - v_prev) ** 2\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "\n",
        "# An example transition.\n",
        "s_prev = jnp.array([1., 2., -1.])\n",
        "r_t = jnp.array(1.)\n",
        "s_t = jnp.array([2., 1., 0.])\n",
        "\n",
        "# Value function and initial parameters\n",
        "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
        "theta = jnp.array([0.1, -0.1, 0.])\n",
        "\n",
        "print(td_update(theta, s_prev, r_t, s_t))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.2 -2.4  1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKTg9m9yz7Ib"
      },
      "source": [
        "## Straight through estimator\n",
        "\n",
        "The straight-through estimator is a trick for defining a 'gradient' of a function that is otherwise non-differentiable. Given a non-differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the backward pass that $f$ is the identity function, so gradients pass through $f$ ignoring the $f'$ term. This can be implemented neatly using `jax.lax.stop_gradient`.\n",
        "\n",
        "Here is an example of a non-differentiable function that converts a soft probability distribution to a one-hot vector (discretization).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFVQKrwqAG4",
        "outputId": "2bd352a0-621f-4cf0-c1ae-6484d5ca1ebf"
      },
      "source": [
        "def onehot(labels, num_classes):\n",
        "  y = (labels[..., None] == jnp.arange(num_classes)[None])\n",
        "  return y.astype(jnp.float32)\n",
        "\n",
        "def quantize(y_soft): \n",
        "  y_hard = onehot(jnp.argmax(y_soft), 3)[0]\n",
        "  return y_hard\n",
        "\n",
        "y_soft = np.array([0.1, 0.2, 0.7])\n",
        "print(quantize(y_soft))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSuQMr61sg16"
      },
      "source": [
        "Now suppose we define some linear function of the quantized variable of the form $f(y) = w^T q(y)$. If $w=[1,2,3]$ and $q(y)=[0,0,1]$, we get $f(y) = 3$. But the gradient is 0 because $q$ is not differentiable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDEMPSdJsTpl",
        "outputId": "11497317-63ea-4c10-cd70-7a95ef2aee44"
      },
      "source": [
        "def f(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  return jnp.dot(w, yq)\n",
        "\n",
        "print(f(y_soft))\n",
        "print(grad(f)(y_soft))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n",
            "[0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTcVC08Rs4DM"
      },
      "source": [
        "To use the straight-through estimator, we replace $q(y)$ with \n",
        "$$y + SG(q(y)-y)$$, where SG is stop gradient. In the forwards pass, we have $y+q(y)-y=q(y)$. In the backwards pass, the gradient of SG is 0, so we effectively replace $q(y)$ with $y$. So in the backwarsd pass we have\n",
        "$$\n",
        "\\begin{align}\n",
        "f(y) &= w^T q(y) \\approx w^T  y \\\\\n",
        "\\nabla_y f(y) &\\approx w\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m_k7Ju3sUVq",
        "outputId": "e3d3a858-cdca-4adf-e3d9-6e60e8bee35a"
      },
      "source": [
        "\n",
        "\n",
        "def f_ste(y):\n",
        "  w = jnp.array([1,2,3])\n",
        "  yq = quantize(y)\n",
        "  yy = y + jax.lax.stop_gradient(yq - y) # gives yq on fwd, and y on backward\n",
        "  return jnp.dot(w, yy)\n",
        "\n",
        "print(f_ste(y_soft))\n",
        "print(grad(f_ste)(y_soft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n",
            "[1. 2. 3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMIYqiOp0U0"
      },
      "source": [
        "## Per-example gradients\n",
        "\n",
        "In some applications, we want to compute the gradient for every example in a batch, not just the sum of gradients over the batch. This is hard in other frameworks like TF and PyTorch but easy in JAX, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhNPiq4qGhl",
        "outputId": "74a2d5a3-e068-4fb8-9f71-d817bc64de9a"
      },
      "source": [
        "def loss(w, x):\n",
        "  return jnp.dot(w,x)\n",
        "\n",
        "w = jnp.ones((3,))\n",
        "x0 = jnp.array([1.0, 2.0, 3.0])\n",
        "x1 = 2*x0\n",
        "X = jnp.stack([x0, x1])\n",
        "print(X.shape)\n",
        "\n",
        "perex_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0)))\n",
        "print(perex_grads(w, X))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxPeXpUaq-_p"
      },
      "source": [
        "To explain the above code in more depth, note that the vmap converts the function loss to take  a batch of inputs for each of its arguments, and returns a batch of outputs. To make it work with a single weight vector, we specify in_axes=(None,0), meaning the first argument (w) is not replicated, and the second argument (x) is replicated along dimension 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYB1L8JKrVKo",
        "outputId": "e7992658-40ae-40b9-be08-09fc32d46c1d"
      },
      "source": [
        "gradfn = jax.grad(loss)\n",
        "\n",
        "W = jnp.stack([w, w])\n",
        "print(jax.vmap(gradfn)(W, X))\n",
        "\n",
        "print(jax.vmap(gradfn, in_axes=(None,0))(w, X))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n",
            "[[1. 2. 3.]\n",
            " [2. 4. 6.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCqo9tWYMxAt"
      },
      "source": [
        "# Optimization\n",
        "\n",
        "The [Optax library](https://github.com/deepmind/optax) implements many common optimizers. Below is a simple example.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUilVmYZOP3E"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install git+git://github.com/deepmind/optax.git\n",
        "import optax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5i2PSK6NcOz",
        "outputId": "c660fde0-6744-4b4b-bea6-5dd671f899b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_weights = 2\n",
        "params = {'w': jnp.ones((num_weights,))}\n",
        "num_ex = 3\n",
        "xs = 2*jnp.ones((num_ex, num_weights))\n",
        "ys = jnp.ones(num_ex)\n",
        "compute_loss_single = lambda params, x, y: optax.l2_loss(params['w'].dot(x), y)\n",
        "compute_loss = lambda params, xs, ys: jnp.sum(jax.vmap(compute_loss_single, in_axes=[None, 0, 0])(params, xs, ys))\n",
        "\n",
        "print('original params ', params)\n",
        "print('loss ', compute_loss(params, xs, ys))\n",
        "\n",
        "# create a stateful optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "print('original state ', opt_state)\n",
        "\n",
        "# compute gradients\n",
        "grads = jax.grad(compute_loss)(params, xs, ys)\n",
        "print('grads ', grads)\n",
        "\n",
        "# updage params (and optstate) given gradients\n",
        "updates, opt_state = optimizer.update(grads, opt_state)\n",
        "params = optax.apply_updates(params, updates)\n",
        "\n",
        "print('updated params ', params)\n",
        "print('updated state ', opt_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original params  {'w': DeviceArray([1., 1.], dtype=float32)}\n",
            "loss  13.5\n",
            "original state  [ScaleByAdamState(count=DeviceArray(0, dtype=int32), mu={'w': DeviceArray([0., 0.], dtype=float32)}, nu={'w': DeviceArray([0., 0.], dtype=float32)}), EmptyState()]\n",
            "grads  {'w': DeviceArray([18., 18.], dtype=float32)}\n",
            "updated params  {'w': DeviceArray([0.9, 0.9], dtype=float32)}\n",
            "updated state  [ScaleByAdamState(count=DeviceArray(1, dtype=int32), mu={'w': DeviceArray([1.8, 1.8], dtype=float32)}, nu={'w': DeviceArray([0.324, 0.324], dtype=float32)}), EmptyState()]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnOIJRGxJigp"
      },
      "source": [
        "\n",
        "# JIT (just in time compilation) <a class=\"anchor\" id=\"JIT\"></a>\n",
        "\n",
        "In this section, we illustrate how to use the Jax JIT compiler to make code go much faster (even on a CPU). It does this by compiling the computational graph into low-level XLA primitives, potentially fusing multiple sequential operations into a single op. However, it does not work on arbitrary Python code, as we explain below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZUbPDLKIkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90339198-30a3-422d-f3ed-6ef07d6ce230"
      },
      "source": [
        "\n",
        "\n",
        "def slow_f(x):\n",
        "  # Element-wise ops see a large benefit from fusion\n",
        "  return x * x + x * 2.0\n",
        "\n",
        "x = jnp.ones((5000, 5000))\n",
        "%timeit  slow_f(x) \n",
        "\n",
        "fast_f = jit(slow_f)\n",
        "%timeit fast_f(x)  \n",
        " \n",
        "assert np.allclose(slow_f(x), fast_f(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 3.11 ms per loop\n",
            "The slowest run took 15.74 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 918 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-33YadKNni"
      },
      "source": [
        "We can also add the `@jit` decorator in front of a function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5L-AEDhAb8t",
        "outputId": "68dce89b-5393-4748-bda0-b730e3fc32d5"
      },
      "source": [
        "@jit\n",
        "def faster_f(x):\n",
        "  return x * x + x * 2.0\n",
        "  \n",
        "%timeit faster_f(x)\n",
        "assert np.allclose(faster_f(x), fast_f(x))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 20.30 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1000 loops, best of 5: 918 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMZVMEOPbLWt"
      },
      "source": [
        "## How it works: Jaxprs and tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvnubTBKbPoN"
      },
      "source": [
        "In this section, we briefly explain the mechanics behind JIT, which will help you understand when it does not work.\n",
        "\n",
        "First, consider this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWK7gOSibbrY"
      },
      "source": [
        "\n",
        "def f(x):\n",
        "  y = jnp.ones((1,5)) * x\n",
        "  return y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkIFd3_lbnaM"
      },
      "source": [
        "When a function is first executed (applied to an argument), it is converted to an intermediate representatio called a JAX expression or jaxpr, by a process called tracing, as we show below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqNGq8z4bxlI",
        "outputId": "ea79ce85-e982-4ec6-eebe-9a7182cd3caf"
      },
      "source": [
        "print(f(3.0))\n",
        "print(jax.make_jaxpr(f)(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 3. 3. 3.]]\n",
            "{ lambda  ; a.\n",
            "  let b = broadcast_in_dim[ broadcast_dimensions=(  )\n",
            "                            shape=(1, 5) ] 1.0\n",
            "      c = convert_element_type[ new_dtype=float32\n",
            "                                weak_type=False ] a\n",
            "      d = mul b c\n",
            "  in (d,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkVzG8Q4cCn5"
      },
      "source": [
        "The XLA JIT compiler can then convert the jaxpr to code that runs fast on a CPU, GPU or TPU; the original python code is no longer needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd9H0391dHGB",
        "outputId": "a3ca889b-2a3d-4127-e79b-ce1f2d177001"
      },
      "source": [
        "f_jit = jit(f)\n",
        "print(f_jit(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3. 3. 3. 3. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAR-o-r_dMvS"
      },
      "source": [
        "However, the jaxpr is created by tracing the function for a specific value. If different code is executed depending on the value of the input arguments, the resulting jaxpr will be different, so the function cannot be JITed, as we illustrate below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "fhH3LaxEbfUh",
        "outputId": "27e19a49-9467-4a05-9d29-7d4e3790d554"
      },
      "source": [
        "\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "print(f(3.0))\n",
        "\n",
        "f_jit = jit(f)\n",
        "print(f_jit(3.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ConcretizationTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-545a2f514f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         donated_invars=donated_invars, inline=inline)\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1604\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1616\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    612\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    619\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 620\u001b[0;31m                                *unsafe_map(arg_spec, args))\n\u001b[0m\u001b[1;32m    621\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    696\u001b[0m   jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(\n\u001b[0;32m--> 697\u001b[0;31m       fun, abstract_args, pe.debug_info_final(fun, \"jit\"))\n\u001b[0m\u001b[1;32m    698\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, debug_info)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-545a2f514f83>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mConcretizationTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at <ipython-input-91-545a2f514f83>:2 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-545a2f514f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf_jit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-91-545a2f514f83>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at <ipython-input-91-545a2f514f83>:2 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOgfVbEePcw"
      },
      "source": [
        "Jit will create a new compiled version for each different ShapedArray, but will reuse the code for different values of the same shape. If the code path depends on the concrete value,  we can either just jit a subfunction (whose code path is constant), or we can create a different jaxpr for each concrete value of the input arguments as we explain below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdtqCDWZAC2f"
      },
      "source": [
        "## Static argnum\n",
        "\n",
        "Note that JIT compilation requires that the control flow through the function  can be determined by the shape (but not concrete value) of its inputs. The function below violates this, since when x<0, it takes one branch, whereas when x>0, it takes the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ps1W8LhKKj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fd7116-5529-4392-be08-7332496deef1"
      },
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "\n",
        "# This will fail!\n",
        "try:\n",
        "  print(f(3))\n",
        "except Exception as e:\n",
        "  print(\"ERROR:\", e)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "The problem arose with the `bool` function. \n",
            "While tracing the function f at <ipython-input-92-94e6eda28128>:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'x'.\n",
            "\n",
            "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tHh4tcXKTRf"
      },
      "source": [
        "We can fix this by telling JAX to trace the control flow through the function using concrete values of some of its arguments. JAX will then compile different versions, depending on the input values. See below for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMaRplccKRHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d449c80d-fe85-4b4c-d4ad-6b9e0e4799ca"
      },
      "source": [
        "\n",
        "\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "f = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f(3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvcgAJEB9Dnn",
        "outputId": "1ddaa1ea-5ea3-4491-9c8a-5c187dd88ed5"
      },
      "source": [
        "@partial(jit, static_argnums=(0,))\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "print(f(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snLung369jTw"
      },
      "source": [
        "## Jit and vmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Fkav2GRgnc"
      },
      "source": [
        "Unfortunately, the static argnum method fails when the function is passed to  vmap, because the latter can take arguments of different shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "QVLpm6n9faJp",
        "outputId": "ff3d967d-953e-4c3b-ef91-b67845c66c91"
      },
      "source": [
        "xs = jnp.arange(5)\n",
        "\n",
        "@partial(jit, static_argnums=(0,))\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "ys = vmap(f)(xs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-00960fe99363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mbatched_fun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflatten_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vmap out_axes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m     ).call_wrapped(*args_flat)\n\u001b[0m\u001b[1;32m   1288\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: ValueError: Non-hashable static arguments are not supported. An error occured while trying to hash an object of type <class 'jax.interpreters.batching.BatchTracer'>, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([0, 1, 2, 3, 4], dtype=int32)\n       batch_dim = 0. The error was:\nTypeError: unhashable type: 'BatchTracer'\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-00960fe99363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported. An error occured while trying to hash an object of type <class 'jax.interpreters.batching.BatchTracer'>, Traced<ShapedArray(int32[])>with<BatchTrace(level=1/0)>\n  with val = DeviceArray([0, 1, 2, 3, 4], dtype=int32)\n       batch_dim = 0. The error was:\nTypeError: unhashable type: 'BatchTracer'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-L-dB_GBWNq"
      },
      "source": [
        "## Side effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zrYp3-KcMc"
      },
      "source": [
        "Since the jaxpr is created only once, if your function has global side-effects, such as using print, they will only happen once, even if the function is called multiple times. See example below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEW-j8TURjri",
        "outputId": "de5d99e6-a423-479b-8a69-ec4b4eabe44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def f(x):\n",
        "  print('x', x)\n",
        "  y = 2 * x\n",
        "  print('y', y)\n",
        "  return y\n",
        "\n",
        "y1 = f(2)\n",
        "print('f', y1)\n",
        "print('\\ncall function a second time')\n",
        "y1 = f(2)\n",
        "print('f', y1)\n",
        "\n",
        "print('\\njit version follows')\n",
        "g = jax.jit(f)\n",
        "y2 = g(2)\n",
        "print('f', y2)\n",
        "\n",
        "print('\\ncall jitted function a second time')\n",
        "y2 = g(2)\n",
        "print('f', y2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x 2\n",
            "y 4\n",
            "f 4\n",
            "\n",
            "call function a second time\n",
            "x 2\n",
            "y 4\n",
            "f 4\n",
            "\n",
            "jit version follows\n",
            "x Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "y Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
            "f 4\n",
            "\n",
            "call jitted function a second time\n",
            "f 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFsfzYHzhbM3"
      },
      "source": [
        "## Caching\n",
        "\n",
        "If you write `g=jax.jit(f)`, then f will get compiled and the XLA code will be cahced. Subsequent calls to g reuse the cached code for speed. But if the jit is called inside a loop, it is effectively making a new f each time, which is slow. So typically jit occurs in the outermost scope (modulo being constant shape).\n",
        "\n",
        "Also, if you specify `static_argnums`, then the cached code will be used only for the same values of arguments labelled as static. If any of them change, recompilation occurs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZAt_DANKlW"
      },
      "source": [
        "## Strings\n",
        "\n",
        "Jit does not work with functions that consume or return strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "rlw0wY3YNPPV",
        "outputId": "d16ea051-806d-4111-d819-6fe01d29bd1a"
      },
      "source": [
        "def f(x: int, y: str):\n",
        "  if y=='add':\n",
        "    return x+1\n",
        "  else:\n",
        "    return x-1\n",
        "\n",
        "print(f(42, 'add'))\n",
        "print(f(42, 'sub'))\n",
        "\n",
        "fj = jax.jit(f)\n",
        "print(fj(42, 'add'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n",
            "41\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-613837d92373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m       \u001b[0m_check_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   2484\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_valid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Argument '{arg}' of type {type(arg)} is not a valid JAX type.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: Argument 'add' of type <class 'str'> is not a valid JAX type.\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-613837d92373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   2483\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_valid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Argument '{arg}' of type {type(arg)} is not a valid JAX type.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[0;31m# TODO(necula): this duplicates code in core.valid_jaxtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument 'add' of type <class 'str'> is not a valid JAX type."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gctKo1NjJzQ1"
      },
      "source": [
        "# Pytrees\n",
        "\n",
        "A Pytree is a container of leaf elements and/or more pytrees. Containers include lists, tuples, and dicts. A leaf element is anything that’s not a pytree, e.g. an array.\n",
        "Pytrees are useful for representing hierarchical sets of parameters for DNNs (and other structured dsta). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGxWcHWJKtsq"
      },
      "source": [
        "## Simple example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1EllUvlJ1NW",
        "outputId": "0b451bd9-bb75-488b-d01a-a3d73225309e"
      },
      "source": [
        "from jax import tree_util\n",
        "\n",
        "# a simple pytree\n",
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "print('tree', t1)\n",
        "leaves = jax.tree_leaves(t1)\n",
        "print('num leaves', len(leaves))\n",
        "print(leaves)\n",
        "\n",
        "t4 = [jnp.array([1, 2, 3]), \"foo\"]\n",
        "print('tree', t4)\n",
        "leaves = jax.tree_leaves(t4)\n",
        "print('num leaves', len(leaves))\n",
        "print(leaves)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree [1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
            "num leaves 5\n",
            "[1, 2, 3, 4, 5]\n",
            "tree [DeviceArray([1, 2, 3], dtype=int32), 'foo']\n",
            "num leaves 2\n",
            "[DeviceArray([1, 2, 3], dtype=int32), 'foo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA0CvQoX4EiW"
      },
      "source": [
        "## Treemap\n",
        "\n",
        "\n",
        "We can map functions down a pytree in the same way that we can map a function down a list. We can also combine elements in two pytrees that have the same shape to make a third pytree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y3UFtJZKr08",
        "outputId": "2f161445-0828-4105-999f-b32dfe99cc21"
      },
      "source": [
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "print(t1)\n",
        "\n",
        "t2 = tree_util.tree_map(lambda x: x*x, t1)\n",
        "print('square each element', t2)\n",
        "\n",
        "\n",
        "t3 = tree_util.tree_map(lambda x,y: x+y, t1, t2)\n",
        "print('t1+t2', t3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
            "square each element [1, {'k1': 4, 'k2': (9, 16)}, 25]\n",
            "t1+t2 [2, {'k1': 6, 'k2': (12, 20)}, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpxCulXHEQ1_"
      },
      "source": [
        "If we have a list of dicts, we can convert to a dict of lists, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9L9EaaLBrPp",
        "outputId": "6400fe69-0a4a-4346-cca4-56f7890107bb"
      },
      "source": [
        "\n",
        "data = [dict(t=1, obs='a', val=-1), dict(t=2, obs='b', val=-2), dict(t=3, obs='c', val=-3)]\n",
        "\n",
        "data2 = jax.tree_map(lambda d0, d1, d2: list((d0, d1, d2)),\n",
        "                          data[0], data[1], data[2])\n",
        "print(data2)\n",
        "\n",
        "def join_trees(list_of_trees):\n",
        "  d = jax.tree_map(lambda *xs: list(xs), *list_of_trees)\n",
        "  return d\n",
        "\n",
        "print(join_trees(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'obs': ['a', 'b', 'c'], 't': [1, 2, 3], 'val': [-1, -2, -3]}\n",
            "{'obs': ['a', 'b', 'c'], 't': [1, 2, 3], 'val': [-1, -2, -3]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvKtqaSu9xj2"
      },
      "source": [
        "## Flattening / Unflattening\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-ixjjrE90BF",
        "outputId": "f85ccac0-da87-4c85-80c7-502880f4bca5"
      },
      "source": [
        "\n",
        "t1 = [1, {\"k1\": 2, \"k2\": (3, 4)}, 5]\n",
        "print(t1)\n",
        "leaves, treedef = jax.tree_util.tree_flatten(t1)\n",
        "print(leaves)\n",
        "print(treedef)\n",
        "\n",
        "t2 = jax.tree_util.tree_unflatten(treedef, leaves)\n",
        "print(t2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]\n",
            "[1, 2, 3, 4, 5]\n",
            "PyTreeDef([*, {'k1': *, 'k2': (*, *)}, *])\n",
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8iMrvUXK8Id"
      },
      "source": [
        "## Example: Linear regression \n",
        "\n",
        "In this section we show how to use pytrees as a container for parameters of a linear reregression model. \n",
        "The code is based on the [flax JAX tutorial](https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html). When we compute the gradient, it will also be a pytree, and will have the same shape as the parameters, so we can add the params to the gradient without having to flatten and unflatten the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77GAXfBmLByE"
      },
      "source": [
        "\n",
        "\n",
        "# Create the predict function from a set of parameters\n",
        "def make_predict_pytree(params):\n",
        "  def predict(x):\n",
        "    return jnp.dot(params['W'],x)+params['b']\n",
        "  return predict\n",
        "\n",
        "# Create the loss from the data points set\n",
        "def make_mse_pytree(x_batched,y_batched): # returns fn(params)->real\n",
        "  def mse(params):\n",
        "    # Define the squared loss for a single pair (x,y)\n",
        "    def squared_error(x,y):\n",
        "      y_pred = make_predict_pytree(params)(x)\n",
        "      return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
        "    # We vectorize the previous to compute the average of the loss on all samples.\n",
        "    return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
        "  return jax.jit(mse) # And finally we jit the result."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7Y8gcM7uhk"
      },
      "source": [
        "# Set problem dimensions\n",
        "N = 20\n",
        "xdim = 10\n",
        "ydim = 5\n",
        "\n",
        "# Generate random ground truth W and b\n",
        "key = random.PRNGKey(0)\n",
        "Wtrue = random.normal(key, (ydim, xdim))\n",
        "btrue = random.normal(key, (ydim,))\n",
        "params_true = {'W': Wtrue, 'b': btrue}\n",
        "true_predict_fun = make_predict_pytree(params_true)\n",
        "\n",
        "# Generate data with additional observation noise\n",
        "X = random.normal(key, (N, xdim))\n",
        "Ytrue = jax.vmap(true_predict_fun)(X)\n",
        "Y = Ytrue + 0.1*random.normal(key, (N, ydim))\n",
        "\n",
        "# Generate MSE for our samples\n",
        "mse_fun = make_mse_pytree(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0YusY89LGS8",
        "outputId": "1f0c943d-b6d4-4ce1-935c-b6997b978b56"
      },
      "source": [
        "# Initialize estimated W and b with zeros.\n",
        "params = {'W': jnp.zeros_like(Wtrue), 'b': jnp.zeros_like(btrue)}\n",
        "\n",
        "mse_pytree = make_mse_pytree(X, Y)\n",
        "print(mse_pytree(params_true))\n",
        "print(mse_pytree(params))\n",
        "print(jax.grad(mse_pytree)(params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.022292053\n",
            "24.97824\n",
            "{'W': DeviceArray([[-0.039,  0.755,  0.542,  0.36 ,  0.224,  1.651,  1.534,\n",
            "              -1.342, -0.15 , -1.638],\n",
            "             [-0.324,  0.141, -0.402,  0.498,  1.829,  4.308,  2.138,\n",
            "              -2.43 , -0.381, -2.178],\n",
            "             [ 1.7  , -0.707, -0.656, -0.568,  1.824, -2.194, -0.477,\n",
            "               0.96 ,  1.622,  1.408],\n",
            "             [-0.862,  0.321, -0.388, -0.74 , -0.82 ,  0.441,  0.772,\n",
            "              -1.713, -1.592, -0.557],\n",
            "             [ 1.338, -0.632, -0.968, -1.127,  1.775,  0.323,  1.405,\n",
            "              -0.638,  1.077, -0.739]], dtype=float32), 'b': DeviceArray([ 0.036,  1.092, -0.413, -1.389, -0.862], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA0ZjMofMFW6",
        "outputId": "6015e963-b57e-4f73-f6b5-b2860427a10a"
      },
      "source": [
        "alpha = 0.3 # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse_pytree(params_true))\n",
        "for i in range(101):\n",
        "  gradients = jax.grad(mse_pytree)(params)\n",
        "  params = jax.tree_map(lambda old,grad: old-alpha*grad, params, gradients)\n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_pytree(params))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for \"true\" W,b:  0.022292053\n",
            "Loss step 0:  6.5597453\n",
            "Loss step 10:  0.17232798\n",
            "Loss step 20:  0.04339735\n",
            "Loss step 30:  0.024473602\n",
            "Loss step 40:  0.017078908\n",
            "Loss step 50:  0.013489487\n",
            "Loss step 60:  0.011695375\n",
            "Loss step 70:  0.010795272\n",
            "Loss step 80:  0.010343453\n",
            "Loss step 90:  0.01011666\n",
            "Loss step 100:  0.010002809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_tNi1V38VAo",
        "outputId": "7ee365cf-c5e6-4201-c8e1-f4b94002fa3b"
      },
      "source": [
        "print(jax.tree_map(lambda x,y: np.allclose(x,y, atol=1e-1), params, params_true))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W': True, 'b': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfHqKwgl9T2Q"
      },
      "source": [
        "Compare the above to what the training code would look like\n",
        "if W and b were passed in as separate arguments:\n",
        "```\n",
        "for i in range(101):\n",
        "  grad_W = jax.grad(mse_fun,0)(What,bhat)\n",
        "  grad_b = jax.grad(mse_fun,1)(What,bhat)\n",
        "  What = What - alpha*grad_W\n",
        "  bhat = bhat - alpha*grad_b \n",
        "  if (i%10==0):\n",
        "    print(\"Loss step {}: \".format(i), mse_fun(What,bhat)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t_pU8lG9xyl"
      },
      "source": [
        "## Example: MLPs\n",
        "\n",
        "We now show a more interesting example, from the Deepmind tutorial, where we fit an MLP using SGD. The basic structure is similar to the linear regression case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8FyAwlf9_Vk"
      },
      "source": [
        "# define the model \n",
        "def init_mlp_params(layer_widths):\n",
        "  params = []\n",
        "  for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "    params.append(\n",
        "        dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n",
        "             biases=np.ones(shape=(n_out,))\n",
        "            )\n",
        "    )\n",
        "  return params\n",
        "\n",
        "def forward(params, x):\n",
        "  *hidden, last = params\n",
        "  for layer in hidden:\n",
        "    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n",
        "  return x @ last['weights'] + last['biases']\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "  return jnp.mean((forward(params, x) - y) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx5EXL79-B6i",
        "outputId": "2f21dca9-8819-4d74-83a3-bc44848918bf"
      },
      "source": [
        "# MLP with 2 hidden layers and linear output\n",
        "np.random.seed(0)\n",
        "params = init_mlp_params([1, 128, 128, 1])\n",
        "jax.tree_map(lambda x: x.shape, params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'biases': (128,), 'weights': (1, 128)},\n",
              " {'biases': (128,), 'weights': (128, 128)},\n",
              " {'biases': (1,), 'weights': (128, 1)}]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ITNC0muv-RiH",
        "outputId": "92234f40-c7b1-41ea-cce3-09a2bec4d20c"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "@jax.jit\n",
        "def update(params, x, y):\n",
        "  grads = jax.grad(loss_fn)(params, x, y)\n",
        "  return jax.tree_map(\n",
        "      lambda p, g: p - LEARNING_RATE * g, params, grads)\n",
        "\n",
        "np.random.seed(0)\n",
        "xs = np.random.normal(size=(200, 1))\n",
        "ys = xs ** 2\n",
        "\n",
        "for _ in range(1000):\n",
        "  params = update(params, xs, ys)\n",
        "\n",
        "plt.scatter(xs, ys, label='truth')\n",
        "plt.scatter(xs, forward(params, xs), label='Prediction')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f91e6397190>"
            ]
          },
          "metadata": {},
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDXJO4FdUPJM"
      },
      "source": [
        "# Looping constructs\n",
        "\n",
        "For loops in Python are slow, even when JIT-compiled. However, there are built-in primitives for loops that are fast, as we illustrate below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wtZiZmU329"
      },
      "source": [
        "## For loops.\n",
        "\n",
        "The semantics of the for loop function in JAX is as follows:\n",
        "```\n",
        "def fori_loop(lower, upper, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(lower, upper):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "```\n",
        "We see that ```val``` is used to accumulate the results across iterations.\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIlLD0kKVGya"
      },
      "source": [
        "# sum from 1 to N = N*(N+1)/2\n",
        "\n",
        "def sum_exact(N):\n",
        "  return int(N*(N+1)/2)\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = 0\n",
        "  for i in range(1,N+1):\n",
        "    s += i\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "\n",
        "assert sum_slow(N) == sum_exact(N)\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,partial_sum: i+partial_sum, 0)\n",
        "  return s\n",
        "\n",
        "assert sum_fast(N) == sum_exact(N) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuOBk9SRLJb",
        "outputId": "4cbdcae2-9fbc-472a-fad0-efb6eee8084c"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 loops, best of 5: 54.8 µs per loop\n",
            "10 loops, best of 5: 21.9 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUsG4mUR3Sl"
      },
      "source": [
        "# Let's do more compute per step of the for loop\n",
        "\n",
        "D = 10\n",
        "X = jax.random.normal(key, shape=(D,D))\n",
        "\n",
        "def sum_slow(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  for i in range(1,N+1):\n",
        "    s += jnp.dot(X, X)\n",
        "  return s\n",
        "\n",
        "def sum_fast(N):\n",
        "  s = jnp.zeros_like(X)\n",
        "  s = jax.lax.fori_loop(1, N+1, lambda i,s: s+jnp.dot(X,X), s)\n",
        "  return s\n",
        "\n",
        "N = 10\n",
        "assert np.allclose(sum_fast(N), sum_slow(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8NtevKvS8F_",
        "outputId": "a24ffd80-ce4b-450b-b1b0-bb681e3cae5b"
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 298 ms per loop\n",
            "10 loops, best of 5: 27.8 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru5D0tZEiV2e"
      },
      "source": [
        "## While loops\n",
        "\n",
        "Here is the semantics of the JAX while loop\n",
        "\n",
        "\n",
        "```\n",
        "def while_loop(cond_fun, body_fun, init_val):\n",
        "  val = init_val\n",
        "  while cond_fun(val):\n",
        "    val = body_fun(val)\n",
        "  return val\n",
        "```\n",
        "\n",
        "Below is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKhfJTDiiSI"
      },
      "source": [
        "\n",
        "\n",
        "def sum_slow_while(N):\n",
        "  s = 0\n",
        "  i = 0\n",
        "  while (i <= N):\n",
        "    s += i\n",
        "    i += 1\n",
        "  return s\n",
        "\n",
        "\n",
        "def sum_fast_while(N):\n",
        "  init_val = (0,0)\n",
        "  def cond_fun(val):\n",
        "    s,i = val\n",
        "    return i<=N\n",
        "  def body_fun(val):\n",
        "    s,i = val\n",
        "    s += i\n",
        "    i += 1\n",
        "    return (s,i)\n",
        "  val = jax.lax.while_loop(cond_fun, body_fun, init_val)\n",
        "  s2 = val[0]\n",
        "  return s2\n",
        "\n",
        "N = 10\n",
        "assert sum_slow_while(N) == sum_exact(N)\n",
        "assert sum_slow_while(N) == sum_fast_while(N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUFiwITkXyOa",
        "outputId": "4664fe5a-53d2-4e34-b1db-583cfbef1778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N = 1000\n",
        "%timeit sum_slow(N)\n",
        "%timeit sum_fast(N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 312 ms per loop\n",
            "10 loops, best of 5: 28.3 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ierIhSOX1Va"
      },
      "source": [
        "## Scan\n",
        "\n",
        "Here is the semantics of scan:\n",
        "```\n",
        "def scan(f, init, xs, length=None):\n",
        "  if xs is None:\n",
        "    xs = [None] * length\n",
        "  carry = init\n",
        "  ys = []\n",
        "  for x in xs:\n",
        "    carry, y = f(carry, x)\n",
        "    ys.append(y)\n",
        "  return carry, np.stack(ys)\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1LvELtJYqGp"
      },
      "source": [
        "Here is an example where we use scan to sample from a discrete-time, discrete-state Markov chain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lcPFKfQYuyT",
        "outputId": "e58d689e-d187-498d-b736-a6d0e98f6d6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "init_dist = jnp.array([0.8, 0.2])\n",
        "trans_mat = jnp.array([[0.9, 0.1], [0.5, 0.5]])\n",
        "rng_key = jax.random.PRNGKey(0)\n",
        "from jax.scipy.special import logit\n",
        "seq_len = 15\n",
        "\n",
        "initial_state = jax.random.categorical(rng_key, logits=logit(init_dist), shape=(1,))\n",
        "\n",
        "def draw_state(prev_state, key):\n",
        "    logits = logit(trans_mat[:, prev_state])\n",
        "    state = jax.random.categorical(key, logits=logits.flatten(), shape=(1,))\n",
        "    return state, state\n",
        "\n",
        "rng_key, rng_state, rng_obs = jax.random.split(rng_key, 3)\n",
        "keys = jax.random.split(rng_state, seq_len - 1)\n",
        "\n",
        "final_state, states = jax.lax.scan(draw_state, initial_state, keys)\n",
        "\n",
        "print(states)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCOVFuZg2QKm"
      },
      "source": [
        "# Common gotchas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JbntckLIflT"
      },
      "source": [
        "## Handling state\n",
        "\n",
        "In this section, we discuss how to transform code that uses object-oriented programming (which can be stateful) to pure functional programming, which is stateless, as required by JAX. Our presentation is based on the Deepmind tutorial.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xhaFAPgJUZl"
      },
      "source": [
        "To start, consider a simple class that maintains an internal counter, and when called, increments the counter and returns the next number from some sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIv0ke8VJXjf",
        "outputId": "534d6510-c218-4f3b-c6e7-42c4dd65c199"
      },
      "source": [
        "#import string\n",
        "#DICTIONARY = list(string.ascii_lowercase)\n",
        "SEQUENCE = jnp.arange(0,100,2)\n",
        "\n",
        "class Counter:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = 0\n",
        "\n",
        "  def count(self) -> int:\n",
        "    #res = DICTIONARY[self.n]\n",
        "    res = SEQUENCE[self.n]\n",
        "    self.n += 1\n",
        "    return res\n",
        "\n",
        "  def reset(self):\n",
        "    self.n = 0\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for _ in range(3):\n",
        "  print(counter.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc5RW-O0LIhy"
      },
      "source": [
        "The trouble with the above code is that the call to `count` depends on the internal state of the object (the value `n`), even though this is not an argument to the function. (The code is therefoe said to violate 'referential transparency'.) When we Jit compile it, Jax will only call the code once (to convert to a jaxpr), so the side effect of updating `n` will not happen, resulting in incorrect behavior, as we show below,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPFnq8ufLjxM",
        "outputId": "a5c06352-feac-4ca4-9059-87f5f9163c15"
      },
      "source": [
        "counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  print(fast_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUeUHAKMMrCv"
      },
      "source": [
        "We can solve this problem by passing the state as an argument into the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spj-8wELMzJV",
        "outputId": "20166198-4315-4300-c4d4-789d35e409f5"
      },
      "source": [
        "CounterState = int\n",
        "Result = int\n",
        "\n",
        "class CounterV2:\n",
        "\n",
        "  def count(self, n: CounterState) -> Tuple[Result, CounterState]:\n",
        "    return SEQUENCE[n], n+1\n",
        "\n",
        "  def reset(self) -> CounterState:\n",
        "    return 0\n",
        "\n",
        "counter = CounterV2()\n",
        "state = counter.reset()\n",
        "\n",
        "for _ in range(3):\n",
        "  value, state = counter.count(state)\n",
        "  print(value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzy9QtLnNy74"
      },
      "source": [
        "This version is functionally pure, so jit-compiles nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGbuYX8MN3mN",
        "outputId": "d876823b-12fc-4dfd-80b4-857e873dac8a"
      },
      "source": [
        "state = counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  value, state = fast_count(state)\n",
        "  print(value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jmTUu5ORwB"
      },
      "source": [
        "\n",
        "We can apply the same process to any stateful method to convert it into a stateless one. We took a class of the form\n",
        "\n",
        "```\n",
        "class StatefulClass\n",
        "\n",
        "  state: State\n",
        "\n",
        "  def stateful_method(*args, **kwargs) -> Output:\n",
        "```\n",
        "\n",
        "and turned it into a class of the form\n",
        "\n",
        "```\n",
        "class StatelessClass\n",
        "\n",
        "  def stateless_method(state: State, *args, **kwargs) -> (Output, State):\n",
        "```\n",
        "\n",
        "This is a common [functional programming](https://en.wikipedia.org/wiki/Functional_programming) pattern, and, essentially, is the way that state is handled in all JAX programs (as we saw with the way Jax handles random number state, or parameters of a model that get updated).\n",
        "Note that the stateless version of the code no longer needs to use a class, but can instead group the functions into a common namespace using modules.\n",
        "\n",
        "In some cases (eg when working with DNNs), it is more convenient to write code in an OO way. There are several libraries (notably [Flax](https://github.com/google/flax) and [Haiku](https://github.com/deepmind/dm-haiku))  that let you define a model in an OO way, and then generate functionally pure code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AieugSOZLGi5"
      },
      "source": [
        "## Mutation of arrays \n",
        "\n",
        "Since JAX is functional, you cannot mutate arrays in place,\n",
        "since this makes program analysis and transformation very difficult. JAX requires a pure functional expression of a numerical program.\n",
        "Instead, JAX offers the functional update functions: `index_update`, `index_add`, `index_min`, `index_max`, and the `index` helper. These are illustrated below. However it is best to avoid these if possible, since they are slow.\n",
        "\n",
        "Note: If the input values of `index_update` aren't reused, jit-compiled code will perform these operations in-place, rather than making a copy. \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKfhvrTLEBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c263bc2-4431-48ee-b805-c89e4bb47b87"
      },
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "A = jnp.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "try:\n",
        "  A[1, :] = 1.0\n",
        "except:\n",
        "  print('must use index_update')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "must use index_update\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P3uUMzXVjw9",
        "outputId": "0dfc73a6-5d63-4364-a0eb-e966bb821b01"
      },
      "source": [
        "from jax.ops import index, index_add, index_update\n",
        "\n",
        "D = 3\n",
        "A = 2*jnp.ones((D,D))\n",
        "print(\"original array:\")\n",
        "print(A)\n",
        "\n",
        "A2 = index_update(A, index[1, :], 42.0) # A[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A2)\n",
        "\n",
        "A3 = A.at[1,:].set(42.0) # A3=np.copy(A),  A3[1,:] = 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A3)\n",
        "\n",
        "A4 = A.at[1,:].mul(42.0) # A4=np.copy(A),  A4[1,:] *= 42\n",
        "print(\"original array:\")\n",
        "print(A) # unchanged\n",
        "print(\"new array:\")\n",
        "print(A4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [42. 42. 42.]\n",
            " [ 2.  2.  2.]]\n",
            "original array:\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 2. 2.]]\n",
            "new array:\n",
            "[[ 2.  2.  2.]\n",
            " [84. 84. 84.]\n",
            " [ 2.  2.  2.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmI9DH2K_nl"
      },
      "source": [
        "## Implicitly casting lists to vectors\n",
        "\n",
        "You cannot treat a list of numbers as a vector. Instead you must explicitly create the vector using the np.array() constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUURw01jKnXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e7610d-cc8a-4fde-8ab7-d80a53ae81ac"
      },
      "source": [
        "# You cannot treat a list of numbers as a vector. \n",
        "try:\n",
        "  S = jnp.diag([1.0, 2.0, 3.0])\n",
        "except:\n",
        "  print('must convert indices to np.array')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "must convert indices to np.array\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMgaegpLCYw"
      },
      "source": [
        "# Instead you should explicitly construct the vector.\n",
        "\n",
        "S = jnp.diag(jnp.array([1.0, 2.0, 3.0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}